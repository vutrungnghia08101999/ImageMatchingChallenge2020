{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "import argparse\n",
    "import copy\n",
    "from copy import deepcopy\n",
    "import datetime\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from eval_metrics import ErrorRateAt95Recall\n",
    "from models import HardNet\n",
    "from dataset import TripletPhotoTour, BrownTest\n",
    "from losses import loss_HardNet\n",
    "from utils import cv2_scale, np_reshape, read_yaml\n",
    "from constants import (\n",
    "    ANCHORAVE,\n",
    "    ANCHORSWAP,\n",
    "    BATCH_SIZE,\n",
    "    BATCH_REDUCE,\n",
    "    DATASET,\n",
    "    DATAROOT,\n",
    "    EPOCHS,\n",
    "    ENVIRONMENT,\n",
    "    EXPERIMENT_NAME,\n",
    "    GPU_ID,\n",
    "    LR_DECAY,\n",
    "    LOSS,\n",
    "    LEARNING_RATE,\n",
    "    LOG_INTERVAL,\n",
    "    MARGIN,\n",
    "    N_TRIPLETS,\n",
    "    NUM_WORKERS,\n",
    "    RESUME,\n",
    "    SEED,\n",
    "    START_EPOCH,\n",
    "    TRAIN_MEAN_IMAGE,\n",
    "    TRAIN_STD_IMAGE,\n",
    "    USE_CUDA,\n",
    "    WEIGHT_DECAY,\n",
    ")\n",
    "\n",
    "logging.basicConfig(filename='logs.txt',\n",
    "                    filemode='a',\n",
    "                    format='%(asctime)s, %(levelname)s: %(message)s',\n",
    "                    datefmt='%y-%m-%d %H:%M:%S',\n",
    "                    level=logging.DEBUG)\n",
    "console = logging.StreamHandler()\n",
    "console.setLevel(logging.INFO)\n",
    "logging.getLogger().addHandler(console)\n",
    "# logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info('\\n\\n================ IMAGE MATCHING CHALLENGE 2020 ==================\\n\\n')\n",
    "configs = read_yaml('configs.yml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_output = os.path.join('models', f'{configs[EXPERIMENT_NAME]}')\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(configs[ENVIRONMENT][GPU_ID])\n",
    "\n",
    "if configs[ENVIRONMENT][USE_CUDA]:\n",
    "    cudnn.benchmark = True\n",
    "    torch.cuda.manual_seed_all(configs[ENVIRONMENT][SEED])\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "random.seed(configs[ENVIRONMENT][SEED])\n",
    "torch.manual_seed(configs[ENVIRONMENT][SEED])\n",
    "np.random.seed(configs[ENVIRONMENT][SEED])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, optimizer, epoch):\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    for batch_id, data in tqdm(enumerate(train_loader)):\n",
    "        data_a, data_p, _ = data\n",
    "\n",
    "        data_a, data_p  = data_a.cuda(), data_p.cuda()\n",
    "        if configs[ENVIRONMENT][USE_CUDA]:\n",
    "            data_a, data_p  = data_a.cuda(), data_p.cuda()\n",
    "            out_a = model(data_a)\n",
    "            out_p = model(data_p)\n",
    "        else:\n",
    "            out_a = model(data_a)\n",
    "            out_p = model(data_p)           \n",
    "\n",
    "        loss = loss_HardNet(out_a, out_p,\n",
    "                        margin=configs[MARGIN],\n",
    "                        anchor_swap=configs[ANCHORSWAP],\n",
    "                        anchor_ave=configs[ANCHORAVE],\n",
    "                        batch_reduce = configs[BATCH_REDUCE],\n",
    "                        loss_type = configs[LOSS])\n",
    "           \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        adjust_learning_rate(optimizer)\n",
    "    \n",
    "        if batch_id%10 == 0:\n",
    "            logging.info(f'{batch_id}/{len(train_loader)} - Loss: {loss.item()}')\n",
    "        \n",
    "    logging.info(f'{len(train_loader)}/{len(train_loader)} - Loss: {loss.item()}')\n",
    "    try:\n",
    "        os.stat(f'{models_output}')\n",
    "    except:\n",
    "        os.makedirs(f'{models_output}')\n",
    "    \n",
    "    x = datetime.datetime.now()\n",
    "    time = x.strftime(\"%y-%m-%d_%H:%M:%S\")\n",
    "    torch.save({'epoch': epoch + 1, 'state_dict': model.state_dict()}, f'{models_output}/checkpoint_{time}_{epoch}.pth')\n",
    "    logging.info(f'{models_output}/checkpoint_{time}_{epoch}.pth')\n",
    "\n",
    "\n",
    "def test(test_loader, model, epoch):\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    labels, distances = [], [] \n",
    "    for (data_a, data_p, label) in tqdm(test_loader):\n",
    "\n",
    "        if configs[ENVIRONMENT][USE_CUDA]:\n",
    "            data_a, data_p = data_a.cuda(), data_p.cuda()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out_a = model(data_a)\n",
    "            out_p = model(data_p)\n",
    "        dists = torch.sqrt(torch.sum((out_a - out_p) ** 2, 1))  # euclidean distance\n",
    "        distances.append(dists.data.cpu().numpy().reshape(-1,1))\n",
    "        ll = label.data.cpu().numpy().reshape(-1, 1)\n",
    "        labels.append(ll)\n",
    "\n",
    "    num_tests = len(test_loader.dataset)\n",
    "    labels = np.vstack(labels).reshape(num_tests)\n",
    "    distances = np.vstack(distances).reshape(num_tests)\n",
    "\n",
    "    fpr95 = ErrorRateAt95Recall(labels, 1.0 / (distances + 1e-8))\n",
    "    logging.info('\\33[91mTest set: Accuracy(FPR95): {:.8f}\\n\\33[0m'.format(fpr95))\n",
    "\n",
    "    return\n",
    "\n",
    "def adjust_learning_rate(optimizer):\n",
    "    \"\"\"Updates the learning rate given the learning rate decay.\n",
    "    The routine has been implemented according to the original Lua SGD optimizer\n",
    "    \"\"\"\n",
    "    for group in optimizer.param_groups:\n",
    "        if 'step' not in group:\n",
    "            group['step'] = 0.\n",
    "        else:\n",
    "            group['step'] += 1.\n",
    "        group['lr'] = configs[LEARNING_RATE] * (\n",
    "        1.0 - float(group['step']) * float(configs[BATCH_SIZE]) / (configs[N_TRIPLETS] * float(configs[EPOCHS])))\n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "def create_transform():\n",
    "    return transforms.Compose([transforms.Lambda(cv2_scale),\n",
    "                               transforms.Lambda(np_reshape),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((configs[DATASET][TRAIN_MEAN_IMAGE],), (configs[DATASET][TRAIN_STD_IMAGE],))])\n",
    "\n",
    "def create_trainloader(root: str, train_scenes: list, n_triplets: int):   \n",
    "    dataset = TripletPhotoTour(root=root,\n",
    "                               transform=create_transform(),\n",
    "                               train_scenes=train_scenes,\n",
    "                               n_triplets=n_triplets)\n",
    "    return DataLoader(dataset, batch_size=configs[BATCH_SIZE], shuffle=False, num_workers=configs[NUM_WORKERS])\n",
    "\n",
    "def create_testloader(root: str, test_scene: str, is_challenge_data: bool):\n",
    "    if is_challenge_data:\n",
    "        dataset = TripletPhotoTour(root=root,\n",
    "                                   transform=create_transform(),\n",
    "                                   test_scene=test_scene,\n",
    "                                   train=False)\n",
    "    else:\n",
    "        dataset = BrownTest(root=root,\n",
    "                            scene=test_scene,\n",
    "                            transform=create_transform())\n",
    "    return DataLoader(dataset, batch_size=configs[BATCH_SIZE], shuffle=False, num_workers=configs[NUM_WORKERS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = create_trainloader(root=configs['dataset']['challenge_root'],\n",
    "                                      train_scenes=configs['dataset']['train_scenes'],\n",
    "                                      n_triplets=configs['n_triplets'])\n",
    "\n",
    "test_dataloaders = []\n",
    "for scene in configs['dataset']['test_scenes']['challenge']:\n",
    "    logging.info(f'load test set: {scene}')\n",
    "    dataloader = create_testloader(root=configs['dataset']['challenge_root'],\n",
    "                                   test_scene=scene,\n",
    "                                   is_challenge_data=True)\n",
    "    test_dataloaders.append((scene, dataloader))\n",
    "\n",
    "for scene in configs['dataset']['test_scenes']['brown']:\n",
    "    logging.info(f'load test set: {scene}')\n",
    "    dataloader = create_testloader(root=configs['dataset']['brown_root'],\n",
    "                                   test_scene=scene,\n",
    "                                   is_challenge_data=False)\n",
    "    test_dataloaders.append((scene, dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HardNet()\n",
    "if configs[ENVIRONMENT][USE_CUDA]:\n",
    "    model = model.cuda()\n",
    "logging.info(configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optimizer = optim.SGD(model.features.parameters(), lr=configs[LEARNING_RATE],\n",
    "                                  momentum=0.9, dampening=0.9,\n",
    "                                  weight_decay=configs[WEIGHT_DECAY])\n",
    "# optionally resume from a checkpoint\n",
    "if configs[RESUME]:\n",
    "    if os.path.isfile(configs[RESUME]):\n",
    "        logging.info('=> loading checkpoint {}'.format(configs[RESUME]))\n",
    "        checkpoint = torch.load(configs[RESUME])\n",
    "        configs[START_EPOCH] = checkpoint['epoch']\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "    else:\n",
    "        logging.info('=> no checkpoint found at {}'.format(configs[RESUME]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = configs[START_EPOCH]\n",
    "end = start + configs[EPOCHS]\n",
    "for epoch in range(start, end):\n",
    "    logging.info(f'epoch: {epoch}')\n",
    "    logging.info(configs['dataset']['train_scenes'])\n",
    "    train(train_dataloader, model, optimizer, epoch)\n",
    "    for tup in test_dataloaders:\n",
    "        logging.info(f'Test on {tup[0]}')\n",
    "        test(tup[1], model, epoch)\n",
    "    \n",
    "    train_dataloader = create_trainloader(root=configs['dataset']['challenge_root'],\n",
    "                                          train_scenes=configs['dataset']['train_scenes'],\n",
    "                                          n_triplets=configs['n_triplets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
