{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "import argparse\n",
    "import copy\n",
    "from copy import deepcopy\n",
    "import datetime\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import PIL\n",
    "import random\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.init\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from eval_metrics import ErrorRateAt95Recall\n",
    "from models import HardNet\n",
    "from dataset import TripletPhotoTour\n",
    "from losses import loss_HardNet\n",
    "from utils import cv2_scale, np_reshape, str2bool\n",
    "\n",
    "# logging.basicConfig(filename='logs.txt',\n",
    "#                     filemode='a',\n",
    "#                     format='%(asctime)s, %(levelname)s: %(message)s',\n",
    "#                     datefmt='%y-%m-%d %H:%M:%S',\n",
    "#                     level=logging.DEBUG)\n",
    "# console = logging.StreamHandler()\n",
    "# console.setLevel(logging.INFO)\n",
    "# logging.getLogger().addHandler(console)\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='PyTorch HardNet')\n",
    "\n",
    "parser.add_argument('--w1bsroot', type=str,\n",
    "                    default='data/sets/wxbs-descriptors-benchmark/code/',\n",
    "                    help='path to dataset')\n",
    "parser.add_argument('--dataroot', type=str,\n",
    "                    default='data/',\n",
    "                    help='path to dataset')\n",
    "parser.add_argument('--model-dir', default='models/',\n",
    "                    help='folder to output model checkpoints')\n",
    "parser.add_argument('--experiment-name', default= 'liberty_train/',\n",
    "                    help='experiment path')\n",
    "parser.add_argument('--training-set', default= 'liberty',\n",
    "                    help='Other options: notredame, yosemite')\n",
    "parser.add_argument('--loss', default= 'triplet_margin',\n",
    "                    help='Other options: softmax, contrastive')\n",
    "parser.add_argument('--batch-reduce', default= 'min',\n",
    "                    help='Other options: average, random, random_global, L2Net')\n",
    "parser.add_argument('--num-workers', default= 0, type=int,\n",
    "                    help='Number of workers to be created')\n",
    "parser.add_argument('--pin-memory',type=bool, default= True,\n",
    "                    help='')\n",
    "parser.add_argument('--anchorave', type=str2bool, default=False,\n",
    "                    help='anchorave')\n",
    "parser.add_argument('--imageSize', type=int, default=32,\n",
    "                    help='the height / width of the input image to network')\n",
    "parser.add_argument('--mean-image', type=float, default=0.443728476019,\n",
    "                    help='mean of train dataset for normalization')\n",
    "parser.add_argument('--std-image', type=float, default=0.20197947209,\n",
    "                    help='std of train dataset for normalization')\n",
    "parser.add_argument('--resume', default='', type=str, metavar='PATH',\n",
    "                    help='path to latest checkpoint (default: none)')\n",
    "parser.add_argument('--start-epoch', default=0, type=int, metavar='N',\n",
    "                    help='manual epoch number (useful on restarts)')\n",
    "parser.add_argument('--epochs', type=int, default=10, metavar='E',\n",
    "                    help='number of epochs to train (default: 10)')\n",
    "parser.add_argument('--anchorswap', type=str2bool, default=True,\n",
    "                    help='turns on anchor swap')\n",
    "parser.add_argument('--batch-size', type=int, default=1024, metavar='BS',\n",
    "                    help='input batch size for training (default: 1024)')\n",
    "parser.add_argument('--test-batch-size', type=int, default=1024, metavar='BST',\n",
    "                    help='input batch size for testing (default: 1024)')\n",
    "parser.add_argument('--n-triplets', type=int, default=5000000, metavar='N',\n",
    "                    help='how many triplets will generate from the dataset')\n",
    "parser.add_argument('--margin', type=float, default=1.0, metavar='MARGIN',\n",
    "                    help='the margin value for the triplet loss function (default: 1.0')\n",
    "parser.add_argument('--gor',type=str2bool, default=False,\n",
    "                    help='use gor')\n",
    "parser.add_argument('--freq', type=float, default=10.0,\n",
    "                    help='frequency for cyclic learning rate')\n",
    "parser.add_argument('--alpha', type=float, default=1.0, metavar='ALPHA',\n",
    "                    help='gor parameter')\n",
    "parser.add_argument('--lr', type=float, default=10.0, metavar='LR',\n",
    "                    help='learning rate (default: 10.0. Yes, ten is not typo)')\n",
    "parser.add_argument('--fliprot', type=str2bool, default=True,\n",
    "                    help='turns on flip and 90deg rotation augmentation')\n",
    "parser.add_argument('--augmentation', type=str2bool, default=False,\n",
    "                    help='turns on shift and small scale rotation augmentation')\n",
    "parser.add_argument('--lr-decay', default=1e-6, type=float, metavar='LRD',\n",
    "                    help='learning rate decay ratio (default: 1e-6')\n",
    "parser.add_argument('--wd', default=1e-4, type=float,\n",
    "                    metavar='W', help='weight decay (default: 1e-4)')\n",
    "# Device options\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='enables CUDA training')\n",
    "parser.add_argument('--gpu-id', default='0', type=str,\n",
    "                    help='id(s) for CUDA_VISIBLE_DEVICES')\n",
    "parser.add_argument('--seed', type=int, default=0, metavar='S',\n",
    "                    help='random seed (default: 0)')\n",
    "parser.add_argument('--log-interval', type=int, default=10, metavar='LI',\n",
    "                    help='how many batches to wait before logging training status')\n",
    "\n",
    "# args = parser.parse_args()\n",
    "args = parser.parse_args([\n",
    "    '--fliprot', 'False',\n",
    "    '--experiment-name', 'liberty_train/',\n",
    "    '--gpu-id', '3',\n",
    "    '--n-triplets', '100000'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using cuda\n"
     ]
    }
   ],
   "source": [
    "checkpoints = os.path.join('models', f'{args.experiment_name}')\n",
    "\n",
    "triplet_flag = False\n",
    "\n",
    "# set the device to use by setting CUDA_VISIBLE_DEVICES env variable in\n",
    "# order to prevent any memory allocation on unused GPUs\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu_id\n",
    "\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "logging.info((\"NOT \" if not args.cuda else \"\") + \"Using cuda\")\n",
    "\n",
    "if args.cuda:\n",
    "    cudnn.benchmark = True\n",
    "    torch.cuda.manual_seed_all(args.seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "# torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# set random seeds\n",
    "random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, optimizer, epoch):\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    pbar = tqdm(enumerate(train_loader))\n",
    "    for batch_idx, data in pbar:\n",
    "\n",
    "        data_a, data_p = data\n",
    "\n",
    "        data_a, data_p  = data_a.cuda(), data_p.cuda()\n",
    "        if args.cuda:\n",
    "            data_a, data_p  = data_a.cuda(), data_p.cuda()\n",
    "            out_a = model(data_a)\n",
    "            out_p = model(data_p)\n",
    "        else:\n",
    "            out_a = model(data_a)\n",
    "            out_p = model(data_p)           \n",
    "\n",
    "        loss = loss_HardNet(out_a, out_p,\n",
    "                        margin=args.margin,\n",
    "                        anchor_swap=args.anchorswap,\n",
    "                        anchor_ave=args.anchorave,\n",
    "                        batch_reduce = args.batch_reduce,\n",
    "                        loss_type = args.loss)\n",
    "           \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        adjust_learning_rate(optimizer)\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            pbar.set_description('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                                 epoch, batch_idx * len(data_a), len(train_loader.dataset),\n",
    "                                 100. * batch_idx / len(train_loader),\n",
    "                                 loss.item()))\n",
    "\n",
    "    try:\n",
    "        os.stat(f'{checkpoints}')\n",
    "    except:\n",
    "        os.makedirs(f'{checkpoints}')\n",
    "    \n",
    "    x = datetime.datetime.now()\n",
    "    time = x.strftime(\"%y-%m-%d_%H:%M:%S\")\n",
    "    torch.save({'epoch': epoch + 1, 'state_dict': model.state_dict()}, f'{checkpoints}/checkpoint_{epoch}_{time}.pth')\n",
    "    logging.info(f'{checkpoints}/checkpoint_{epoch}_{time}.pth')\n",
    "\n",
    "\n",
    "def test(test_loader, model, epoch):\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    labels, distances = [], []\n",
    "\n",
    "    pbar = tqdm(enumerate(test_loader))\n",
    "    for batch_idx, (data_a, data_p, label) in pbar:\n",
    "\n",
    "        if args.cuda:\n",
    "            data_a, data_p = data_a.cuda(), data_p.cuda()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out_a = model(data_a)\n",
    "            out_p = model(data_p)\n",
    "        dists = torch.sqrt(torch.sum((out_a - out_p) ** 2, 1))  # euclidean distance\n",
    "        distances.append(dists.data.cpu().numpy().reshape(-1,1))\n",
    "        ll = label.data.cpu().numpy().reshape(-1, 1)\n",
    "        labels.append(ll)\n",
    "\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            pbar.set_description(' Test Epoch: {} [{}/{} ({:.0f}%)]'.format(\n",
    "                epoch, batch_idx * len(data_a), len(test_loader.dataset),\n",
    "                       100. * batch_idx / len(test_loader)))\n",
    "\n",
    "    num_tests = test_loader.dataset.matches.size(0)\n",
    "    labels = np.vstack(labels).reshape(num_tests)\n",
    "    distances = np.vstack(distances).reshape(num_tests)\n",
    "\n",
    "    fpr95 = ErrorRateAt95Recall(labels, 1.0 / (distances + 1e-8))\n",
    "    logging.info('\\33[91mTest set: Accuracy(FPR95): {:.8f}\\n\\33[0m'.format(fpr95))\n",
    "\n",
    "    return\n",
    "\n",
    "def adjust_learning_rate(optimizer):\n",
    "    \"\"\"Updates the learning rate given the learning rate decay.\n",
    "    The routine has been implemented according to the original Lua SGD optimizer\n",
    "    \"\"\"\n",
    "    for group in optimizer.param_groups:\n",
    "        if 'step' not in group:\n",
    "            group['step'] = 0.\n",
    "        else:\n",
    "            group['step'] += 1.\n",
    "        group['lr'] = args.lr * (\n",
    "        1.0 - float(group['step']) * float(args.batch_size) / (args.n_triplets * float(args.epochs)))\n",
    "    return\n",
    "\n",
    "def create_dataloader(name: str, is_train: bool, load_random_triplet: bool):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Lambda(cv2_scale),\n",
    "        transforms.Lambda(np_reshape),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((args.mean_image,), (args.std_image,))])\n",
    "    \n",
    "    dataset = TripletPhotoTour(n_triplets=args.n_triplets,\n",
    "                               fliprot = args.fliprot,\n",
    "                               train=is_train,\n",
    "                               load_random_triplets = load_random_triplet,\n",
    "                               batch_size=args.batch_size,\n",
    "                               root=args.dataroot,\n",
    "                               name=name,\n",
    "                               transform=transform)\n",
    "    return DataLoader(dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 100000 triplets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:03<00:00, 26433.60it/s]\n"
     ]
    }
   ],
   "source": [
    "liberty_dataloader = create_dataloader(name='liberty',\n",
    "                                       is_train=True,\n",
    "                                       load_random_triplet=False)\n",
    "notredame_dataloader = create_dataloader(name='notredame',\n",
    "                                       is_train=False,\n",
    "                                       load_random_triplet=False)\n",
    "yosemite_dataloader = create_dataloader(name='yosemite',\n",
    "                                       is_train=False,\n",
    "                                       load_random_triplet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/namvh/Workspace/hardnet/code_tmp/models.py:52: UserWarning: nn.init.orthogonal is now deprecated in favor of nn.init.orthogonal_.\n",
      "  nn.init.orthogonal(m.weight.data, gain=0.6)\n",
      "INFO:root:\n",
      "parsed options:\n",
      "{'w1bsroot': 'data/sets/wxbs-descriptors-benchmark/code/', 'dataroot': 'data/', 'model_dir': 'models/', 'experiment_name': 'liberty_train/', 'training_set': 'liberty', 'loss': 'triplet_margin', 'batch_reduce': 'min', 'num_workers': 0, 'pin_memory': True, 'anchorave': False, 'imageSize': 32, 'mean_image': 0.443728476019, 'std_image': 0.20197947209, 'resume': '', 'start_epoch': 0, 'epochs': 10, 'anchorswap': True, 'batch_size': 1024, 'test_batch_size': 1024, 'n_triplets': 100000, 'margin': 1.0, 'gor': False, 'freq': 10.0, 'alpha': 1.0, 'lr': 10.0, 'fliprot': False, 'augmentation': False, 'lr_decay': 1e-06, 'wd': 0.0001, 'no_cuda': False, 'gpu_id': '3', 'seed': 0, 'log_interval': 10, 'cuda': True}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = HardNet()\n",
    "if args.cuda:\n",
    "    model = model.cuda()\n",
    "logging.info('\\nparsed options:\\n{}\\n'.format(vars(args)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optimizer = optim.SGD(model.features.parameters(), lr=args.lr,\n",
    "                                  momentum=0.9, dampening=0.9,\n",
    "                                  weight_decay=args.wd)\n",
    "# optionally resume from a checkpoint\n",
    "if args.resume:\n",
    "    if os.path.isfile(args.resume):\n",
    "        logging.info('=> loading checkpoint {}'.format(args.resume))\n",
    "        checkpoint = torch.load(args.resume)\n",
    "        args.start_epoch = checkpoint['epoch']\n",
    "        checkpoint = torch.load(args.resume)\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "    else:\n",
    "        logging.info('=> no checkpoint found at {}'.format(args.resume))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [92160/100000 (92%)]\tLoss: 0.981797: : 98it [01:37,  1.01it/s]\n",
      "INFO:root:models/liberty_train//checkpoint_0_20-03-24_11:06:48.pth\n",
      " Test Epoch: 0 [30720/100000 (31%)]: : 37it [00:13,  2.72it/s]"
     ]
    }
   ],
   "source": [
    "start = args.start_epoch\n",
    "end = start + args.epochs\n",
    "for epoch in range(start, end):\n",
    "    train(liberty_dataloader, model, optimizer, epoch)\n",
    "    test(notredame_dataloader, model, epoch)\n",
    "    test(yosemite_dataloader, model, epoch)\n",
    "    \n",
    "    liberty_dataloader = create_dataloader(name='liberty',\n",
    "                                           is_train=True,\n",
    "                                           load_random_triplet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
