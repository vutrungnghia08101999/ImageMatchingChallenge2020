{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "import argparse\n",
    "import copy\n",
    "from copy import deepcopy\n",
    "import datetime\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import PIL\n",
    "import random\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from eval_metrics import ErrorRateAt95Recall\n",
    "from models import HardNet\n",
    "from dataset import TripletPhotoTour\n",
    "from losses import loss_HardNet\n",
    "from utils import cv2_scale, np_reshape, read_yaml\n",
    "from constants import (\n",
    "    AUGMENTATION,\n",
    "    ANCHORAVE,\n",
    "    ANCHORSWAP,\n",
    "    BATCH_SIZE,\n",
    "    BATCH_REDUCE,\n",
    "    DATASET,\n",
    "    DATAROOT,\n",
    "    EPOCHS,\n",
    "    ENVIRONMENT,\n",
    "    EXPERIMENT_NAME,\n",
    "    FLIPROT,\n",
    "    GPU_ID,\n",
    "    LR_DECAY,\n",
    "    LOSS,\n",
    "    LEARNING_RATE,\n",
    "    LOG_INTERVAL,\n",
    "    MODEL_DIR,\n",
    "    MARGIN,\n",
    "    N_TRIPLETS,\n",
    "    NUM_WORKERS,\n",
    "    RESUME,\n",
    "    SEED,\n",
    "    START_EPOCH,\n",
    "    SET_1,\n",
    "    SET_2,\n",
    "    TRAIN_MEAN_IMAGE,\n",
    "    TRAIN_STD_IMAGE,\n",
    "    TRAINING_SET,\n",
    "    TEST_SET,\n",
    "    TEST_BATCH_SIZE,\n",
    "    USE_CUDA,\n",
    "    WEIGHT_DECAY,\n",
    ")\n",
    "\n",
    "# logging.basicConfig(filename='logs.txt',\n",
    "#                     filemode='a',\n",
    "#                     format='%(asctime)s, %(levelname)s: %(message)s',\n",
    "#                     datefmt='%y-%m-%d %H:%M:%S',\n",
    "#                     level=logging.DEBUG)\n",
    "# console = logging.StreamHandler()\n",
    "# console.setLevel(logging.INFO)\n",
    "# logging.getLogger().addHandler(console)\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = read_yaml('configs.yml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_output = os.path.join('models', f'{configs[EXPERIMENT_NAME]}')\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(configs[ENVIRONMENT][GPU_ID])\n",
    "\n",
    "if configs[ENVIRONMENT][USE_CUDA]:\n",
    "    cudnn.benchmark = True\n",
    "    torch.cuda.manual_seed_all(configs[ENVIRONMENT][SEED])\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "random.seed(configs[ENVIRONMENT][SEED])\n",
    "torch.manual_seed(configs[ENVIRONMENT][SEED])\n",
    "np.random.seed(configs[ENVIRONMENT][SEED])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, optimizer, epoch):\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    pbar = tqdm(enumerate(train_loader))\n",
    "    for batch_idx, data in pbar:\n",
    "\n",
    "        data_a, data_p = data\n",
    "\n",
    "        data_a, data_p  = data_a.cuda(), data_p.cuda()\n",
    "        if configs[ENVIRONMENT][USE_CUDA]:\n",
    "            data_a, data_p  = data_a.cuda(), data_p.cuda()\n",
    "            out_a = model(data_a)\n",
    "            out_p = model(data_p)\n",
    "        else:\n",
    "            out_a = model(data_a)\n",
    "            out_p = model(data_p)           \n",
    "\n",
    "        loss = loss_HardNet(out_a, out_p,\n",
    "                        margin=configs[MARGIN],\n",
    "                        anchor_swap=configs[ANCHORSWAP],\n",
    "                        anchor_ave=configs[ANCHORAVE],\n",
    "                        batch_reduce = configs[BATCH_REDUCE],\n",
    "                        loss_type = configs[LOSS])\n",
    "           \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        adjust_learning_rate(optimizer)\n",
    "        if batch_idx % configs[ENVIRONMENT][LOG_INTERVAL] == 0:\n",
    "            pbar.set_description('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                                 epoch, batch_idx * len(data_a), len(train_loader.dataset),\n",
    "                                 100. * batch_idx / len(train_loader),\n",
    "                                 loss.item()))\n",
    "\n",
    "    try:\n",
    "        os.stat(f'{models_output}')\n",
    "    except:\n",
    "        os.makedirs(f'{models_output}')\n",
    "    \n",
    "    x = datetime.datetime.now()\n",
    "    time = x.strftime(\"%y-%m-%d_%H:%M:%S\")\n",
    "    torch.save({'epoch': epoch + 1, 'state_dict': model.state_dict()}, f'{models_output}/checkpoint_{epoch}_{time}.pth')\n",
    "    logging.info(f'{models_output}/checkpoint_{epoch}_{time}.pth')\n",
    "\n",
    "\n",
    "def test(test_loader, model, epoch):\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    labels, distances = [], []\n",
    "\n",
    "    pbar = tqdm(enumerate(test_loader))\n",
    "    for batch_idx, (data_a, data_p, label) in pbar:\n",
    "\n",
    "        if configs[ENVIRONMENT][USE_CUDA]:\n",
    "            data_a, data_p = data_a.cuda(), data_p.cuda()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out_a = model(data_a)\n",
    "            out_p = model(data_p)\n",
    "        dists = torch.sqrt(torch.sum((out_a - out_p) ** 2, 1))  # euclidean distance\n",
    "        distances.append(dists.data.cpu().numpy().reshape(-1,1))\n",
    "        ll = label.data.cpu().numpy().reshape(-1, 1)\n",
    "        labels.append(ll)\n",
    "\n",
    "        if batch_idx % configs[ENVIRONMENT][LOG_INTERVAL] == 0:\n",
    "            pbar.set_description(' Test Epoch: {} [{}/{} ({:.0f}%)]'.format(\n",
    "                epoch, batch_idx * len(data_a), len(test_loader.dataset),\n",
    "                       100. * batch_idx / len(test_loader)))\n",
    "\n",
    "    num_tests = test_loader.dataset.matches.size(0)\n",
    "    labels = np.vstack(labels).reshape(num_tests)\n",
    "    distances = np.vstack(distances).reshape(num_tests)\n",
    "\n",
    "    fpr95 = ErrorRateAt95Recall(labels, 1.0 / (distances + 1e-8))\n",
    "    logging.info('\\33[91mTest set: Accuracy(FPR95): {:.8f}\\n\\33[0m'.format(fpr95))\n",
    "\n",
    "    return\n",
    "\n",
    "def adjust_learning_rate(optimizer):\n",
    "    \"\"\"Updates the learning rate given the learning rate decay.\n",
    "    The routine has been implemented according to the original Lua SGD optimizer\n",
    "    \"\"\"\n",
    "    for group in optimizer.param_groups:\n",
    "        if 'step' not in group:\n",
    "            group['step'] = 0.\n",
    "        else:\n",
    "            group['step'] += 1.\n",
    "        group['lr'] = configs[LEARNING_RATE] * (\n",
    "        1.0 - float(group['step']) * float(configs[BATCH_SIZE]) / (configs[N_TRIPLETS] * float(configs[EPOCHS])))\n",
    "    return\n",
    "\n",
    "def create_dataloader(name: str, is_train: bool, load_random_triplet: bool):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Lambda(cv2_scale),\n",
    "        transforms.Lambda(np_reshape),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((configs[DATASET][TRAIN_MEAN_IMAGE],), (configs[DATASET][TRAIN_STD_IMAGE],))])\n",
    "    \n",
    "    dataset = TripletPhotoTour(n_triplets=configs[N_TRIPLETS],\n",
    "                               fliprot = configs[FLIPROT],\n",
    "                               train=is_train,\n",
    "                               load_random_triplets = load_random_triplet,\n",
    "                               batch_size=configs[BATCH_SIZE],\n",
    "                               root=configs[DATASET][DATAROOT],\n",
    "                               name=name,\n",
    "                               transform=transform)\n",
    "    return DataLoader(dataset, batch_size=configs[BATCH_SIZE], shuffle=False, num_workers=configs[NUM_WORKERS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 100000 triplets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:13<00:00, 7295.04it/s]\n"
     ]
    }
   ],
   "source": [
    "liberty_dataloader = create_dataloader(name='liberty',\n",
    "                                       is_train=True,\n",
    "                                       load_random_triplet=False)\n",
    "notredame_dataloader = create_dataloader(name='notredame',\n",
    "                                       is_train=False,\n",
    "                                       load_random_triplet=False)\n",
    "yosemite_dataloader = create_dataloader(name='yosemite',\n",
    "                                       is_train=False,\n",
    "                                       load_random_triplet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:{'dataset': {'dataroot': 'data/', 'model_dir': 'models/', 'train_mean_image': 0.443728476019, 'train_std_image': 0.20197947209, 'training_set': 'liberty', 'test_set': {'set_1': 'notredame', 'set_2': 'yosemite'}}, 'experiment_name': 'liberty_train/', 'loss': 'triplet_margin', 'batch_reduce': 'min', 'num_workers': 0, 'anchorave': False, 'resume': '', 'start_epoch': 0, 'epochs': 10, 'anchorswap': True, 'batch_size': 1024, 'test_batch_size': 1024, 'n_triplets': 100000, 'margin': 1.0, 'learning_rate': 10.0, 'fliprot': False, 'augmentation': False, 'lr_decay': 1e-06, 'weight_decay': 0.0001, 'environment': {'use_cuda': True, 'gpu_id': 3, 'seed': 0, 'log_interval': 10}}\n"
     ]
    }
   ],
   "source": [
    "model = HardNet()\n",
    "if configs[ENVIRONMENT][USE_CUDA]:\n",
    "    model = model.cuda()\n",
    "logging.info(configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optimizer = optim.SGD(model.features.parameters(), lr=configs[LEARNING_RATE],\n",
    "                                  momentum=0.9, dampening=0.9,\n",
    "                                  weight_decay=configs[WEIGHT_DECAY])\n",
    "# optionally resume from a checkpoint\n",
    "if configs[RESUME]:\n",
    "    if os.path.isfile(configs[RESUME]):\n",
    "        logging.info('=> loading checkpoint {}'.format(configs[RESUME]))\n",
    "        checkpoint = torch.load(configs[RESUME])\n",
    "        configs[START_EPOCH] = checkpoint['epoch']\n",
    "        checkpoint = torch.load(configs[RESUME])\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "    else:\n",
    "        logging.info('=> no checkpoint found at {}'.format(configs[RESUME]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [92160/100000 (92%)]\tLoss: 0.982789: : 98it [02:36,  1.60s/it]\n",
      "INFO:root:models/liberty_train//checkpoint_0_20-03-24_15:02:07.pth\n",
      " Test Epoch: 0 [92160/100000 (92%)]: : 98it [01:26,  1.13it/s]\n",
      "INFO:root:\u001b[91mTest set: Accuracy(FPR95): 0.14484000\n",
      "\u001b[0m\n",
      " Test Epoch: 0 [92160/100000 (92%)]: : 98it [01:29,  1.09it/s]\n",
      "INFO:root:\u001b[91mTest set: Accuracy(FPR95): 0.20056000\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 100000 triplets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:13<00:00, 7174.02it/s]\n",
      "Train Epoch: 1 [92160/100000 (92%)]\tLoss: 0.936369: : 98it [02:31,  1.54s/it]\n",
      "INFO:root:models/liberty_train//checkpoint_1_20-03-24_15:07:53.pth\n",
      " Test Epoch: 1 [92160/100000 (92%)]: : 98it [01:31,  1.08it/s]\n",
      "INFO:root:\u001b[91mTest set: Accuracy(FPR95): 0.08918000\n",
      "\u001b[0m\n",
      " Test Epoch: 1 [92160/100000 (92%)]: : 98it [01:21,  1.20it/s]\n",
      "INFO:root:\u001b[91mTest set: Accuracy(FPR95): 0.14312000\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 100000 triplets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:12<00:00, 8246.28it/s]\n",
      "Train Epoch: 2 [92160/100000 (92%)]\tLoss: 0.907229: : 98it [02:33,  1.57s/it]\n",
      "INFO:root:models/liberty_train//checkpoint_2_20-03-24_15:13:36.pth\n",
      " Test Epoch: 2 [92160/100000 (92%)]: : 98it [01:26,  1.14it/s]\n",
      "INFO:root:\u001b[91mTest set: Accuracy(FPR95): 0.06630000\n",
      "\u001b[0m\n",
      " Test Epoch: 2 [92160/100000 (92%)]: : 98it [01:28,  1.10it/s]\n",
      "INFO:root:\u001b[91mTest set: Accuracy(FPR95): 0.11666000\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 100000 triplets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:15<00:00, 6373.15it/s]\n",
      "Train Epoch: 3 [40960/100000 (41%)]\tLoss: 0.882545: : 45it [01:11,  1.62s/it]"
     ]
    }
   ],
   "source": [
    "start = configs[START_EPOCH]\n",
    "end = start + configs[EPOCHS]\n",
    "for epoch in range(start, end):\n",
    "    train(liberty_dataloader, model, optimizer, epoch)\n",
    "    test(notredame_dataloader, model, epoch)\n",
    "    test(yosemite_dataloader, model, epoch)\n",
    "    \n",
    "    liberty_dataloader = create_dataloader(name='liberty',\n",
    "                                           is_train=True,\n",
    "                                           load_random_triplet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
